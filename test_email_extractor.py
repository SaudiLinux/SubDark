#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù†Øµ Ø§Ø®ØªØ¨Ø§Ø±ÙŠ Ù„Ø£Ø¯Ø§Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
"""

import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import urljoin, urlparse

class EmailExtractor:
    def __init__(self):
        self.emails = set()
        self.visited_urls = set()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def extract_emails_from_text(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† Ø§Ù„Ù†Øµ"""
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, text)
        return [email for email in emails if self.is_valid_email(email)]
    
    def is_valid_email(self, email):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ"""
        # ØªØµÙÙŠØ© Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ù…Ø¤Ù‚Øª ÙˆØ§Ù„ØºÙŠØ± Ø­Ù‚ÙŠÙ‚ÙŠ
        temp_domains = [
            'tempmail', 'temp-mail', '10minutemail', 'guerrillamail',
            'mailinator', 'throwaway', 'fakeemail', 'tempmailo'
        ]
        
        domain = email.split('@')[1].lower() if '@' in email else ''
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø¤Ù‚Øª
        for temp_domain in temp_domains:
            if temp_domain in domain:
                return False
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙŠØºØ© Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
        if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
            return False
        
        return True
    
    def extract_from_mailto_links(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† Ø±ÙˆØ§Ø¨Ø· mailto"""
        mailto_links = soup.find_all('a', href=re.compile(r'^mailto:', re.I))
        emails = []
        
        for link in mailto_links:
            href = link.get('href', '')
            if 'mailto:' in href:
                email = href.replace('mailto:', '').split('?')[0]
                if self.is_valid_email(email):
                    emails.append(email)
        
        return emails
    
    def extract_from_input_fields(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„"""
        emails = []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù…Ù† Ù†ÙˆØ¹ email
        email_inputs = soup.find_all('input', {'type': 'email'})
        for input_field in email_inputs:
            if 'value' in input_field.attrs:
                email = input_field['value']
                if self.is_valid_email(email):
                    emails.append(email)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø¨Ø£Ø³Ù…Ø§Ø¡ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ email
        email_name_inputs = soup.find_all('input', {'name': re.compile(r'email', re.I)})
        for input_field in email_name_inputs:
            if 'value' in input_field.attrs and input_field['value']:
                email = input_field['value']
                if self.is_valid_email(email):
                    emails.append(email)
        
        return emails
    
    def extract_from_comments(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª HTML"""
        comments = soup.find_all(string=lambda text: isinstance(text, str) and text.strip().startswith('<!--'))
        emails = []
        
        for comment in comments:
            comment_emails = self.extract_emails_from_text(str(comment))
            emails.extend(comment_emails)
        
        return emails
    
    def extract_from_hidden_elements(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ÙÙŠØ©"""
        hidden_elements = soup.find_all(style=re.compile(r'display:\s*none', re.I))
        emails = []
        
        for element in hidden_elements:
            text = element.get_text()
            element_emails = self.extract_emails_from_text(text)
            emails.extend(element_emails)
        
        return emails
    
    def extract_emails_from_url(self, url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† URL Ù…Ø¹ÙŠÙ†"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
            text_emails = self.extract_emails_from_text(soup.get_text())
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø±ÙˆØ§Ø¨Ø· mailto
            mailto_emails = self.extract_from_mailto_links(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            input_emails = self.extract_from_input_fields(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
            comment_emails = self.extract_from_comments(soup)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ÙÙŠØ©
            hidden_emails = self.extract_from_hidden_elements(soup)
            
            # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
            all_emails = text_emails + mailto_emails + input_emails + comment_emails + hidden_emails
            
            return list(set(all_emails))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† {url}: {e}")
            return []
    
    def get_internal_links(self, url, soup):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
        internal_links = []
        base_domain = urlparse(url).netloc
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(url, href)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ
            if urlparse(full_url).netloc == base_domain:
                internal_links.append(full_url)
        
        return list(set(internal_links))
    
    def extract_emails_from_website(self, base_url, max_pages=5):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print(f"Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ù…Ù†: {base_url}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_emails = self.extract_emails_from_url(base_url)
        self.emails.update(main_emails)
        
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
            response = self.session.get(base_url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            internal_links = self.get_internal_links(base_url, soup)
            
            print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(internal_links)} Ø±Ø§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠ")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Ø¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ max_pages)
            pages_crawled = 0
            for link in internal_links[:max_pages]:
                if link not in self.visited_urls:
                    self.visited_urls.add(link)
                    
                    print(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù†: {link}")
                    page_emails = self.extract_emails_from_url(link)
                    self.emails.update(page_emails)
                    
                    pages_crawled += 1
                    time.sleep(1)  # ØªØ£Ø®ÙŠØ± Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
            
            print(f"ØªÙ… Ø²ÙŠØ§Ø±Ø© {pages_crawled} ØµÙØ­Ø© Ø¯Ø§Ø®Ù„ÙŠØ©")
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©: {e}")
        
        return list(self.emails)
    
    def save_emails_to_file(self, emails, filename="extracted_emails.txt"):
        """Ø­ÙØ¸ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙÙŠ Ù…Ù„Ù"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:\n")
                f.write("=" * 50 + "\n")
                
                for email in sorted(emails):
                    f.write(f"{email}\n")
                
                f.write(f"\nØ§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(emails)} Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ\n")
                f.write(f"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            print(f"ØªÙ… Ø­ÙØ¸ {len(emails)} Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙÙŠ Ù…Ù„Ù: {filename}")
            return True
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù: {e}")
            return False

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ"""
    print("=" * 60)
    print("Ø£Ø¯Ø§Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ")
    print("=" * 60)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ù…Ù„Ù HTML Ù…Ø­Ù„ÙŠ
    test_file = "test_email_page.html"
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ù…Ù„Ù: {test_file}")
        print("-" * 40)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        extractor = EmailExtractor()
        emails = extractor.extract_emails_from_text(html_content)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø±ÙˆØ§Ø¨Ø· mailto
        soup = BeautifulSoup(html_content, 'html.parser')
        mailto_emails = extractor.extract_from_mailto_links(soup)
        emails.extend(mailto_emails)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø­Ù‚ÙˆÙ„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
        input_emails = extractor.extract_from_input_fields(soup)
        emails.extend(input_emails)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        comment_emails = extractor.extract_from_comments(soup)
        emails.extend(comment_emails)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø®ÙÙŠØ©
        hidden_emails = extractor.extract_from_hidden_elements(soup)
        emails.extend(hidden_emails)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        unique_emails = list(set(emails))
        
        if unique_emails:
            print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(unique_emails)} Ø¹Ù†ÙˆØ§Ù† Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ:")
            for email in sorted(unique_emails):
                print(f"  ğŸ“§ {email}")
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if unique_emails:
            extractor.save_emails_to_file(unique_emails)
        
    except FileNotFoundError:
        print(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù: {test_file}")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±!")
    print("=" * 60)

if __name__ == "__main__":
    main()